# TABA Pipeline Integration for BiblIA

## ğŸ“– Overview

**TABA (Text Alignment for Building Annotations)** is an external pipeline integrated into BiblIA that automatically generates Ground Truth for training OCR models by aligning known digital texts with OCR results using Passim.

**Original Project:** https://github.com/Freymat/from_eScriptorium_to_Passim_and_back

## ğŸ—ï¿½? Architecture

TABA runs as an **external standalone pipeline**, not embedded inside BiblIA/eScriptorium. BiblIA provides a **management interface** to:
- Manage Ground Truth corpora
- Create and monitor alignment jobs
- Export XML to TABA
- Import aligned results back

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
ï¿½?         BiblIA (Django App)          â”‚
ï¿½?  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ï¿½?  ï¿½?
ï¿½?  ï¿½?  TABA Management Interface     ï¿½?  â”‚
ï¿½?  ï¿½?  - Corpus management           ï¿½?  â”‚
ï¿½?  ï¿½?  - Job creation/monitoring     ï¿½?  â”‚
ï¿½?  ï¿½?  - XML export/import           ï¿½?  ï¿½?
ï¿½?  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ï¿½?  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ï¿½?â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               ï¿½? Export XML (API)
               â†“
ï¿½?â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
ï¿½?  TABA External Pipeline              â”‚
ï¿½?  (Conda environment: Python 3.11)    â”‚
ï¿½?  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ï¿½?  â”‚
ï¿½?  ï¿½? 1. Prepare OCR data            ï¿½?  â”‚
ï¿½?  ï¿½? 2. Run Passim alignment        ï¿½?  â”‚
ï¿½?  ï¿½? 3. Process results             ï¿½?  â”‚
ï¿½?  ï¿½? 4. Generate aligned XMLs       ï¿½?  â”‚
ï¿½?  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ï¿½?  â”‚
ï¿½?â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ï¿½?â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               ï¿½? Import XML (API)
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
ï¿½?  BiblIA: New GT Transcription Layers â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Installation (Follow Official TABA Documentation)

### Step 1: Create TABA Conda Environment

```bash
# Navigate to project root
cd /path/to/BiblIA_dataset/eScriptorium_CLEAN

# Create dedicated TABA directory (parallel to app/)
mkdir -p taba_external
cd taba_external

# Create conda environment
conda create -n alignment_pipeline python=3.11
conda activate alignment_pipeline

# Install Passim
pip install git+https://github.com/dasmiq/passim.git

# Clone TABA repository
git clone https://github.com/Freymat/from_eScriptorium_to_Passim_and_back.git .

# Install requirements
pip install -r requirements.txt
```

### Step 2: Configure TABA

Create `credentials.py`:
```python
# eScriptorium API credentials
root_url = "http://localhost:8082"  # BiblIA URL
headers = {
    "Authorization": "Token YOUR_API_TOKEN_HERE"
}
headersbrief = headers
```

Create/Edit `config.py`:
```python
# Connection to eScriptorium
eSc_connexion = True

# Document to process (will be set by BiblIA jobs)
doc_pk = None  # Set dynamically by BiblIA
region_type_pk_list = []
transcription_level_pk = None

# Passim parameters
n = 7  # Character n-gram order
n_cores = 6  # Number of CPU cores
mem = 8  # Memory in GB
driver_mem = 4  # Driver memory in GB

# Filtering
levenshtein_threshold = 0.8  # Minimum similarity ratio

# Results
n_best_gt = 5  # Top GT texts to display
```

### Step 3: Prepare Ground Truth Texts

```bash
# Create directory for your GT corpus
mkdir -p data/raw/digital_editions

# Add your Hebrew texts (TXT files)
# Example structure:
# data/raw/digital_editions/
#   â”œâ”€â”€ tanakh_genesis.txt
#   ï¿½?â”€â”€ tanakh_exodus.txt
#   â”œâ”€â”€ talmud_berachot.txt
#   ï¿½?â”€â”€ ...
```

### Step 4: Set Environment Variable

Add to your `.env` or `docker-compose.yml`:

```bash
# Path to TABA external pipeline
TABA_PIPELINE_PATH=/path/to/BiblIA/taba_external

# Conda environment name
TABA_CONDA_ENV=alignment_pipeline
```

### Step 5: Run Django Migrations

```bash
# From BiblIA app directory
python manage.py makemigrations taba_pipeline
python manage.py migrate taba_pipeline
```

## ğŸš€ Usage

### 1. Access TABA Dashboard

Navigate to: `http://localhost:8082/taba/`

### 2. Create Ground Truth Corpus

1. Go to **Corpus Management**
2. Click **Add Corpus**
3. Fill in:
   - Name: e.g., "Sefaria Hebrew Bible"
   - Source Type: TXT, PDF, or Sefaria API
   - Source Path: `/path/to/taba_external/data/raw/digital_editions/`

### 3. Create Alignment Job

1. Go to **Jobs** ï¿½? **Create New Job**
2. Select:
   - **Document**: Your eScriptorium document
   - **OCR Transcription**: The Kraken OCR layer
   - **GT Corpus**: Your ground truth corpus
   - **Parameters**:
     - Passim n-grams: 7 (default)
     - Cores: 6
     - Memory: 8 GB
     - Levenshtein threshold: 0.8

### 4. Run the Pipeline

Click **Start Job** ï¿½? TABA will:
1. Export XML from eScriptorium
2. Prepare data for Passim
3. Run Passim alignment
4. Process results
5. Import aligned XMLs back to eScriptorium

### 5. View Results

- **Job Details**: See alignment statistics
- **eScriptorium Document**: New transcription layers created (one per GT text)
- **Reports**: TSV files with detailed metrics

## ğŸ“Š How It Works

### Pipeline Steps (from TABA documentation)

1. **XML Export**: BiblIA exports ALTO XMLs with OCR results
2. **Data Preparation**: 
   - Extract OCR lines from XMLs
   - Load GT texts from corpus
   - Create JSON input for Passim
3. **Passim Alignment**:
   - Find text reuse between OCR and GT
   - Use n-gram matching
4. **Results Processing**:
   - Filter alignments by Levenshtein distance
   - Replace OCR lines with aligned GT (if valid)
   - Empty lines without alignment
5. **XML Import**: Import modified XMLs back to eScriptorium
6. **Transcription Layers**: New layer created for each GT source

### Example Alignment

**Original OCR:**
```
×”×’×“×•×œ ×”×’×‘×•×¨ ×•×“× ×•×¨× ××œ ×¢×œ×™×•×Ÿ ×§×•× ×” ×‘×¨×—××™×•
```

**Aligned GT (Levenshtein ratio: 0.861):**
```
×”×’×“×•×œ ×”×’×‘×•×¨ ×•×”× ×•×¨×. ××œ ×¢×œ×™×•×Ÿ ×§×•× ×”
```

ï¿½? GT replaces OCR in new transcription layer!

## ğŸ”§ Manual TABA Execution (Advanced)

If needed, you can run TABA directly:

```bash
# Activate TABA environment
conda activate alignment_pipeline
cd /path/to/taba_external

# Run full pipeline
python main.py --run_all --no_import

# Or run steps separately:
python main.py --prepare_data_for_passim
python main.py --compute_alignments_with_passim
python main.py --create_xmls_from_passim_results
python main.py --export_xmls_to_eSc
```

## ğŸ“ Directory Structure

```
BiblIA_dataset/eScriptorium_CLEAN/
ï¿½?â”€â”€ app/
ï¿½?   â”œâ”€â”€ apps/
ï¿½?   ï¿½?   â””â”€â”€ taba_pipeline/          # Django management app
ï¿½?   ï¿½?       â”œâ”€â”€ models.py            # DB: Corpus, Jobs, Results
ï¿½?   ï¿½?       ï¿½?â”€â”€ views.py             # UI for TABA management
ï¿½?   ï¿½?       â”œâ”€â”€ admin.py             # Django admin integration
ï¿½?   ï¿½?       â””â”€â”€ templates/           # Dashboard templates
ï¿½?   ï¿½?â”€â”€ escriptorium/
ï¿½?       â””â”€â”€ settings.py              # TABA_PIPELINE_PATH config
â”‚
â””â”€â”€ taba_external/                   # External TABA pipeline
    â”œâ”€â”€ main.py                      # Main execution script
    â”œâ”€â”€ config.py                    # Configuration
    ï¿½?â”€â”€ credentials.py               # API tokens
    â”œâ”€â”€ requirements.txt             # Python dependencies
    â”œâ”€â”€ src/                         # Pipeline modules
    ï¿½?   ï¿½?â”€â”€ fetch_xmls_from_eSc.py
    ï¿½?   â”œâ”€â”€ prepare_data_for_passim.py
    ï¿½?   â”œâ”€â”€ comxxxxxalignments_with_passim.py
    ï¿½?   ï¿½?â”€â”€ process_alignment_results.py
    ï¿½?   â””â”€â”€ export_results_to_eSc.py
    â””â”€â”€ data/
        â”œâ”€â”€ raw/
        ï¿½?   â”œâ”€â”€ xmls_from_eSc/       # Exported from BiblIA
        ï¿½?   â””â”€â”€ digital_editions/    # Your GT texts (TXT)
        â”œâ”€â”€ processed/
        â””â”€â”€ output/
            â””â”€â”€ xmls_for_eSc/        # Aligned XMLs to import
```

## âš™ï¸ Configuration Options

### Passim Parameters

- `n`: N-gram size (default: 7 for Hebrew)
- `n_cores`: CPU cores for Spark
- `mem`: Memory per node (GB)
- `driver_mem`: Driver memory (GB)

### Quality Filtering

- `levenshtein_threshold`: 0.0-1.0 (default: 0.8)
  - Higher = stricter matching
  - Lower = more alignments but less accurate

### Results Display

- `n_best_gt`: Number of top GT texts in summary

## ğŸ› Troubleshooting

### Passim Not Installed
```bash
conda activate alignment_pipeline
pip install git+https://github.com/dasmiq/passim.git
```

### Memory Issues
Reduce in `config.py`:
```python
mem = 4  # Instead of 8
n_cores = 4  # Instead of 6
```

### No Alignments Found
- Check Levenshtein threshold (try lowering to 0.7)
- Verify GT texts are in correct language
- Ensure OCR quality is reasonable

## ğŸ“š References

- **TABA GitHub**: https://github.com/Freymat/from_eScriptorium_to_Passim_and_back
- **Passim**: https://github.com/dasmiq/passim
- **Sefaria API**: https://www.sefaria.org/texts (for Hebrew texts)

## ğŸ¯ Key Benefits

ï¿½? **Automated GT Generation**: No manual transcription needed
ï¿½? **Large Scale**: Process thousands of pages automatically
ï¿½? **Quality Controlled**: Levenshtein filtering ensures accuracy
ï¿½? **BiblIA Integrated**: Seamless workflow with eScriptorium
ï¿½? **External Pipeline**: Doesn't modify BiblIA core code

---

**Integrated:** 26 October 2025
**Original Author:** Freymat (MiDRASH Project - EPHE)
**BiblIA Integration:** BiblIA Development Team
