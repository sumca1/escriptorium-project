# ×ª×›× ×•×Ÿ API ×¤×˜×¤×˜×Ÿ - ××‘×•×¡×¡ TaskReport ×××™×ª×™
## 22 ××•×§×˜×•×‘×¨ 2025

### âœ… ×¢×§×¨×•×Ÿ: ×œ× ×œ×”××¦×™× - ×¨×§ ×œ×”×¦×™×’!

**××§×•×¨ ×”×××ª:** `reporting_taskreport.messages` (55KB+ ×©×œ ××–×”×¨×•×ª!)

---

## ğŸ¯ Endpoint ×—×“×©: GET /api/tasks/{id}/detailed/

**××˜×¨×”:** ×œ×”×¦×™×’ ××ª ×”-messages ×‘×¦×•×¨×” ××•×‘× ×™×ª ×•×§×¨×™××”

### ×ª×©×•×‘×” ×œ×“×•×’××”:

```json
{
  "task_id": 506,
  "status": "error",
  "status_display": "×§×¨×¡",
  "method": "imports.tasks.document_import",
  "document": {
    "id": 8,
    "name": "Pinkas Training Set"
  },
  "timing": {
    "queued_at": "2025-10-22T17:15:00Z",
    "started_at": "2025-10-22T17:15:05Z",
    "done_at": "2025-10-22T17:15:48Z",
    "duration_seconds": 43,
    "cpu_cost": 0.0095,
    "gpu_cost": null
  },
  "summary": {
    "total_warnings": 48,
    "total_errors": 2,
    "lines_without_baseline": 45,
    "auto_added_block_types": ["paragraph", "other", "signature-mark"]
  },
  "warnings": [
    {
      "type": "missing_baseline",
      "file": "Page 105_1.xml",
      "line_number": 11,
      "message": "×©×•×¨×” ×œ×œ× ×§×• ×‘×¡×™×¡ ×‘-Page 105_1.xml ×©×•×¨×” ××¡' 11, ×¡×‘×™×¨ ×××•×“ ×©×œ× ×ª×”×™×” ×©××™×©×”!"
    },
    {
      "type": "missing_baseline",
      "file": "Page 105_1.xml",
      "line_number": 107,
      "message": "×©×•×¨×” ×œ×œ× ×§×• ×‘×¡×™×¡ ×‘-Page 105_1.xml ×©×•×¨×” ××¡' 107, ×¡×‘×™×¨ ×××•×“ ×©×œ× ×ª×”×™×” ×©××™×©×”!"
    }
    // ... 43 more
  ],
  "errors": [
    {
      "type": "duplicate_blocktype",
      "message": "get() returned more than one BlockType -- it returned 2!"
    },
    {
      "type": "duplicate_blocktype",  
      "message": "get() returned more than one BlockType -- it returned 2!"
    }
  ],
  "impact_assessment": {
    "severity": "medium",
    "usable_data_percentage": 93.8,
    "recommendation": "×¨×•×‘ ×”× ×ª×•× ×™× ×ª×§×™× ×™×. 45 ×©×•×¨×•×ª ×œ×œ× baseline ×œ× ×™×©××©×• ×œ××™××•×Ÿ. ×©×§×•×œ ×œ× ×§×•×ª XMLs ××• ×œ×ª×§×Ÿ ×™×“× ×™×ª."
  },
  "next_steps": [
    "×‘×“×•×§ ××ª ×”×©×•×¨×•×ª ×¢× ×‘×¢×™×•×ª ×‘×¢×•×¨×š",
    "×× ×¨×•×¦×” ×œ×××Ÿ - ×™×© ××¡×¤×™×§ × ×ª×•× ×™× (93.8%)",
    "×©×§×•×œ ×œ×ª×§×Ÿ XMLs ×œ×©×™×¤×•×¨ ××™×›×•×ª"
  ]
}
```

---

## ğŸ”§ ×™×™×©×•×

### 1. Parser ×œ-messages

```python
# app/apps/api/message_parser.py

import re
from typing import List, Dict, Any

class TaskMessageParser:
    """Parse TaskReport.messages into structured data"""
    
    @staticmethod
    def parse(messages: str) -> Dict[str, Any]:
        """
        Parse raw messages string into structured warnings/errors
        
        Returns:
            {
                "warnings": [...],
                "errors": [...],
                "summary": {...}
            }
        """
        lines = messages.split('\n')
        
        warnings = []
        errors = []
        auto_added_blocks = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # Parse: "×©×•×¨×” ×œ×œ× ×§×• ×‘×¡×™×¡ ×‘-Page 105_1.xml ×©×•×¨×” ××¡' 11, ×¡×‘×™×¨..."
            baseline_match = re.search(
                r'×©×•×¨×” ×œ×œ× ×§×• ×‘×¡×™×¡ ×‘-(.+\.xml) ×©×•×¨×” ××¡\' (\d+)',
                line
            )
            if baseline_match:
                warnings.append({
                    'type': 'missing_baseline',
                    'file': baseline_match.group(1),
                    'line_number': int(baseline_match.group(2)),
                    'message': line
                })
                continue
            
            # Parse: "×¡×•×’ ×‘×œ×•×§ paragraph × ×•×¡×£ ××•×˜×•××˜×™×ª ×œ××•× ×˜×•×œ×•×’×™×”"
            block_match = re.search(
                r'×¡×•×’ ×‘×œ×•×§ (.+) × ×•×¡×£ ××•×˜×•××˜×™×ª ×œ××•× ×˜×•×œ×•×’×™×”',
                line
            )
            if block_match:
                block_type = block_match.group(1)
                if block_type not in auto_added_blocks:
                    auto_added_blocks.append(block_type)
                warnings.append({
                    'type': 'auto_added_block',
                    'block_type': block_type,
                    'message': line
                })
                continue
            
            # Parse: "get() returned more than one BlockType -- it returned 2!"
            if 'BlockType' in line or 'returned more than' in line:
                errors.append({
                    'type': 'duplicate_blocktype',
                    'message': line
                })
                continue
            
            # Generic error/warning
            if 'error' in line.lower() or '×©×’×™××”' in line:
                errors.append({'type': 'generic', 'message': line})
            else:
                warnings.append({'type': 'generic', 'message': line})
        
        # Calculate stats
        missing_baseline_count = sum(
            1 for w in warnings if w['type'] == 'missing_baseline'
        )
        
        return {
            'warnings': warnings,
            'errors': errors,
            'summary': {
                'total_warnings': len(warnings),
                'total_errors': len(errors),
                'lines_without_baseline': missing_baseline_count,
                'auto_added_block_types': auto_added_blocks
            }
        }
```

### 2. Enhanced Serializer

```python
# app/apps/api/serializers.py

from api.message_parser import TaskMessageParser

class DetailedTaskReportSerializer(serializers.ModelSerializer):
    """Detailed task report with parsed messages"""
    
    status_display = serializers.CharField(source='get_workflow_state_display')
    document_id = serializers.IntegerField(source='document.id', allow_null=True)
    document_name = serializers.CharField(source='document.name', allow_null=True)
    
    timing = serializers.SerializerMethodField()
    parsed_messages = serializers.SerializerMethodField()
    impact_assessment = serializers.SerializerMethodField()
    
    class Meta:
        model = TaskReport
        fields = [
            'id', 'label', 'method', 'workflow_state', 'status_display',
            'document_id', 'document_name',
            'timing', 'parsed_messages', 'impact_assessment'
        ]
    
    def get_timing(self, obj):
        duration = None
        if obj.started_at and obj.done_at:
            duration = (obj.done_at - obj.started_at).total_seconds()
            
        return {
            'queued_at': obj.queued_at.isoformat() if obj.queued_at else None,
            'started_at': obj.started_at.isoformat() if obj.started_at else None,
            'done_at': obj.done_at.isoformat() if obj.done_at else None,
            'duration_seconds': duration,
            'cpu_cost': obj.cpu_cost,
            'gpu_cost': obj.gpu_cost
        }
    
    def get_parsed_messages(self, obj):
        """Parse messages into structured data"""
        if not obj.messages:
            return {'warnings': [], 'errors': [], 'summary': {}}
        
        return TaskMessageParser.parse(obj.messages)
    
    def get_impact_assessment(self, obj):
        """Assess impact of warnings/errors"""
        parsed = self.get_parsed_messages(obj)
        
        total_warnings = parsed['summary']['total_warnings']
        total_errors = parsed['summary']['total_errors']
        missing_baselines = parsed['summary'].get('lines_without_baseline', 0)
        
        # Simple heuristic for severity
        if total_errors > 5:
            severity = 'high'
        elif total_errors > 0 or missing_baselines > 100:
            severity = 'medium'
        elif missing_baselines > 20:
            severity = 'low'
        else:
            severity = 'minimal'
        
        # Estimate usable data (rough calculation)
        # Assume ~50 lines per part, 24 parts = 1200 lines
        # missing_baselines won't be used
        estimated_total = 1200
        usable = max(0, estimated_total - missing_baselines)
        usable_percentage = (usable / estimated_total * 100) if estimated_total > 0 else 0
        
        recommendation = self._get_recommendation(severity, usable_percentage, total_errors)
        
        return {
            'severity': severity,
            'usable_data_percentage': round(usable_percentage, 1),
            'recommendation': recommendation
        }
    
    def _get_recommendation(self, severity, usable_pct, errors):
        if severity == 'high':
            return "×™×© ×‘×¢×™×•×ª ×¨×¦×™× ×™×•×ª. ××•××œ×¥ ×œ×‘×“×•×§ ×•×œ×ª×§×Ÿ ×œ×¤× ×™ ×”××©×š."
        elif severity == 'medium':
            if usable_pct > 90:
                return f"×¨×•×‘ ×”× ×ª×•× ×™× ×ª×§×™× ×™× ({usable_pct}%). × ×™×ª×Ÿ ×œ×”××©×™×š ×œ××™××•×Ÿ."
            else:
                return f"×™×© ×‘×¢×™×•×ª. ×¨×§ {usable_pct}% ××”× ×ª×•× ×™× ×©××™×©×™×. ×©×§×•×œ ×ª×™×§×•×Ÿ."
        else:
            return "×”× ×ª×•× ×™× ×‘××™×›×•×ª ×˜×•×‘×”. × ×™×ª×Ÿ ×œ×”××©×™×š."
```

### 3. View

```python
# app/apps/api/views.py

class DetailedTaskReportView(RetrieveAPIView):
    """
    Get detailed task report with parsed messages
    
    GET /api/tasks/{id}/detailed/
    """
    permission_classes = [IsAuthenticated]
    serializer_class = DetailedTaskReportSerializer
    queryset = TaskReport.objects.all()
```

### 4. URL

```python
# app/apps/api/urls.py

path('tasks/<int:pk>/detailed/', 
     DetailedTaskReportView.as_view(), 
     name='task-detailed'),
```

---

## ğŸ§ª ×©×™××•×©

```powershell
# ×‘×“×™×§×ª ×™×™×‘×•× Document 8
$taskId = 506  # ××–×”×” TaskReport
$result = Invoke-RestMethod `
    -Uri "http://localhost:8082/api/tasks/$taskId/detailed/" `
    -Headers @{ 'Authorization' = "Token $token" }

# ×”×¦×’ ×¡×™×›×•×
Write-Host "Severity: $($result.impact_assessment.severity)"
Write-Host "Usable Data: $($result.impact_assessment.usable_data_percentage)%"
Write-Host "Warnings: $($result.parsed_messages.summary.total_warnings)"
Write-Host "Errors: $($result.parsed_messages.summary.total_errors)"
```

---

## âœ… ×™×ª×¨×•× ×•×ª ×”×’×™×©×” ×”×–×•

1. **×××™×ª×™** - ×§×•×¨× ×-TaskReport, ×œ× ×××¦×™×
2. **××•×‘× ×”** - JSON ×§×¨×™× ×‘××§×•× text blob
3. **×©×™××•×©×™** - impact_assessment + recommendations
4. **×”×¨×—×‘×”** - ×§×œ ×œ×”×•×¡×™×£ parsers ×œ××¡×¨×™× × ×•×¡×¤×™×
5. **×¤×˜×¤×˜×Ÿ** - ××¡×‘×™×¨ ××” ×§×¨×” ×•××” ×œ×¢×©×•×ª

---

## ğŸš€ ××” ×”×œ××”?

1. **×œ×™×™×©× ××ª ×–×”!**
2. **×œ×‘×“×•×§ ×¢× ×™×™×‘×•× ×××™×ª×™**
3. **×œ×”×•×¡×™×£ parsers ×œ××¡×¨×™× × ×•×¡×¤×™×** (train, segtrain, etc.)
4. **×œ×©×¤×¨ recommendations** (ML-based?)
5. **×œ×”×•×¡×™×£ WebSocket** ×œ×¢×“×›×•× ×™× ×‘×–××Ÿ ×××ª
